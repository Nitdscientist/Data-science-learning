{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac38a514-250f-4a23-a4fe-ed070602a8f2",
   "metadata": {},
   "source": [
    "# Assignment-21\n",
    "\n",
    "## NLP\n",
    "\n",
    "### Name- NITESH KUMAR Batch-4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db4025a-6a6d-4404-8966-2f5516e3ce16",
   "metadata": {},
   "source": [
    "# Q1. What is the primary purpose of tokenization in natural language processing? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885ab6fe-20ee-453b-b020-b71fda09e4be",
   "metadata": {},
   "source": [
    "### Answer:-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c06f81-129c-40f6-b570-f659bc3de0fb",
   "metadata": {},
   "source": [
    "In natural language processing (NLP), tokenization is the process of breaking down a text into smaller units called tokens. These tokens can be words, phrases, sentences, or other meaningful elements, depending on the level of granularity required for a particular NLP task. The primary purpose of tokenization is to convert raw text into a format that can be easily processed and analyzed by machines.\n",
    "\n",
    "#### Here are some key reasons for tokenization in NLP:\n",
    "\n",
    "1. Text Preprocessing:\n",
    "Tokenization is often the first step in text preprocessing. Breaking down text into tokens makes it easier to handle and apply subsequent processing steps such as stemming, lemmatization, and stop-word removal.\n",
    "\n",
    "2. Feature Extraction:\n",
    "Tokens serve as the basic building blocks for feature extraction in NLP tasks. By representing text as a sequence of tokens, machine learning models can analyze and learn patterns from the data more effectively.\n",
    "\n",
    "3. Text Representation:\n",
    "Tokenization is crucial for converting text into a format that can be used for various NLP applications, such as sentiment analysis, named entity recognition, and machine translation. Each token represents a discrete unit of meaning.\n",
    "\n",
    "4. Vocabulary Building:\n",
    "Tokenization is essential for building the vocabulary of a corpus. Each unique token typically corresponds to a unique word, and by keeping track of the vocabulary, models can understand the distribution of words and learn relationships between them.\n",
    "\n",
    "5. Text Analysis:\n",
    "Tokenization enables the analysis of the structure and content of a text. It helps in understanding the syntactic and semantic aspects of language, which is important for tasks like part-of-speech tagging and parsing.\n",
    "\n",
    "6. Computational Efficiency:\n",
    "Working with tokens rather than raw text improves computational efficiency. It simplifies the processing of text data, making it more manageable and reducing the computational resources required for subsequent tasks.\n",
    "\n",
    "7. Machine Learning Input:\n",
    "Many machine learning models, especially those used in NLP, require fixed-size input vectors. Tokenization helps convert variable-length sequences of text into a format suitable for feeding into these models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6ba4ad-b455-49e0-95ae-5eccfb5bee37",
   "metadata": {},
   "source": [
    "# Q2. Write a Python function to tokenize a given sentence? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25a2c84-50b7-4573-98c2-4c525c4dfc8b",
   "metadata": {},
   "source": [
    "### Answer:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6dfd29a-d78f-4584-ac09-b7e424d6ef55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m56.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from nltk) (4.64.1)\n",
      "Collecting regex>=2021.8.3\n",
      "  Downloading regex-2023.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (773 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m773.9/773.9 kB\u001b[0m \u001b[31m55.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from nltk) (1.2.0)\n",
      "Installing collected packages: regex, nltk\n",
      "Successfully installed nltk-3.8.1 regex-2023.10.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5477e8e9-45c4-4be4-b019-53ffa14e9ef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt')  # Download the necessary data for tokenization\n",
    "\n",
    "def tokenize_sentence(sentence):\n",
    "    tokens = word_tokenize(sentence)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c959751c-93c6-444e-8dae-f13b9d2090b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tokenization', 'is', 'an', 'important', 'step', 'in', 'natural', 'language', 'processing', '.']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Tokenization is an important step in natural language processing.\"\n",
    "tokens = tokenize_sentence(sentence)\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35763857-a147-429c-8477-dc8b4682b033",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fe151a3a-9a77-45a5-ab0c-e7a50abcf43d",
   "metadata": {},
   "source": [
    "# Q3. Why is the removal of stop words considered significant in NLP? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a64732-2a33-413f-952c-45fdee05b88b",
   "metadata": {},
   "source": [
    "### Answer:-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d92eb5c-7f64-47cb-a87d-86dbc0df99a1",
   "metadata": {},
   "source": [
    "The removal of stop words is considered significant in natural language processing (NLP) for several reasons:\n",
    "\n",
    "1. Reduction of Dimensionality:\n",
    "Stop words are commonly occurring words in a language that often do not carry significant meaning by themselves, such as articles (e.g., \"the,\" \"a,\" \"an\"), prepositions (e.g., \"in,\" \"on,\" \"at\"), and conjunctions (e.g., \"and,\" \"but,\" \"or\"). By removing these words, the overall dimensionality of the data is reduced, making it easier to analyze and process.\n",
    "\n",
    "2. Improved Computational Efficiency:\n",
    "Stop words are frequently used, but they typically contribute less to the overall meaning of a document. Removing them can significantly reduce the size of the data that needs to be processed, leading to improved computational efficiency. This is especially important when working with large datasets.\n",
    "\n",
    "3. Focus on Content Words:\n",
    "Stop word removal allows NLP models to focus more on content words, which carry the primary meaning of a text. Content words, such as nouns, verbs, and adjectives, are often more informative for tasks like sentiment analysis, topic modeling, and document classification.\n",
    "\n",
    "4. Enhanced Semantic Analysis:\n",
    "Stop words often do not contribute much to the semantic content of a document. By eliminating them, the remaining words become more indicative of the underlying meaning, facilitating more accurate semantic analysis.\n",
    "\n",
    "5. Improved Information Retrieval:\n",
    "In tasks related to information retrieval, such as document retrieval or search engines, the removal of stop words can improve the precision and relevance of search results. Queries and documents can be compared based on more meaningful terms.\n",
    "\n",
    "6. Prevention of Data Skewness:\n",
    "Stop words are highly frequent in language and can dominate term frequency statistics. Removing them helps prevent skewness in the data, ensuring that less informative words do not disproportionately influence the results of certain analyses.\n",
    "\n",
    "7. Memory and Storage Efficiency:\n",
    "By excluding stop words from the analysis, memory and storage requirements are reduced. This is particularly beneficial when working with resource-constrained environments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14edca4-7cba-4bb9-8680-11e22f559141",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b806afd7-8803-4e0d-b1fc-e2c242c2d51b",
   "metadata": {},
   "source": [
    "# Q4. Write a Python function to eliminate stop words from a given sentence. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6286c77-5614-4ad5-9a9b-a8dbbd181310",
   "metadata": {},
   "source": [
    "### Answer:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f033279-35c9-4e38-b810-e71a5bf45b41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "def remove_stop_words(sentence):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = word_tokenize(sentence)\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    filtered_sentence = ' '.join(filtered_words)\n",
    "    return filtered_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e15c3b29-d635-4f02-8483-c61b088b04e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing stop words essential natural language processing .\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Removing stop words is essential for natural language processing.\"\n",
    "result = remove_stop_words(sentence)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9200120b-48a4-426b-8467-b5ca14b5d133",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7b09d36d-ab8e-487d-bf3a-5d6a56e8d9cc",
   "metadata": {},
   "source": [
    "# Q5. Explain the key differences between stemming and lemmatization in text processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0595ad6e-ea89-4553-b66e-8b3bda1bc0fb",
   "metadata": {},
   "source": [
    "### Answer:-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fdbc38-afc0-49bf-8eb2-c223be71e0fd",
   "metadata": {},
   "source": [
    "Stemming and lemmatization are two text processing techniques used in natural language processing (NLP) to reduce words to their base or root forms. However, they differ in their approaches and the results they produce. Here are the key differences between stemming and lemmatization:\n",
    "\n",
    "## Stemming:\n",
    "\n",
    "#### Definition:\n",
    "\n",
    "It is the process of removing suffixes from words to obtain their root forms. The goal is to reduce words to a common base form, even if the result is not a valid word in the language.\n",
    "#### Method:\n",
    "\n",
    "It uses heuristics or rule-based approaches to chop off suffixes. It doesn't always guarantee that the resulting stem is a valid word.\n",
    "#### Output:\n",
    "\n",
    "The output (stem) may not be a real word. For example, \"running\" might be stemmed to \"run,\" even though \"run\" is a valid word.\n",
    "#### Speed:\n",
    "\n",
    "It tends to be faster computationally because it involves simple rule-based operations.\n",
    "### Examples:\n",
    "\n",
    "Stemming:\n",
    "\"running\" -> \"run\"\n",
    "\"happily\" -> \"happi\"\n",
    "\"better\" -> \"better\"\n",
    "\n",
    "## Lemmatization:\n",
    "\n",
    "#### Definition:\n",
    "\n",
    "It is the process of reducing words to their base or dictionary form, known as the lemma. The goal is to transform words into valid words by considering their meaning in context.\n",
    "#### Method:\n",
    "\n",
    "It involves using language dictionaries and morphological analysis to obtain the base or dictionary form of a word.\n",
    "#### Output:\n",
    "\n",
    "The output (lemma) is always a valid word. For example, \"running\" might be lemmatized to \"run,\" and \"better\" might be lemmatized to \"good.\"\n",
    "#### Precision:\n",
    "\n",
    "It is more precise than stemming because it considers the context and meaning of words. It requires more complex linguistic rules and tools.\n",
    "### Examples:\n",
    "\n",
    "\"running\" -> \"run\"\n",
    "\"happily\" -> \"happy\"\n",
    "\"better\" -> \"good\"\n",
    "\n",
    "In summary, stemming aims to reduce words to a common root, often resulting in non-words, while lemmatization aims to reduce words to their base or dictionary form, ensuring that the output is always a valid word. The choice between stemming and lemmatization depends on the specific requirements of the NLP task and the desired trade-off between simplicity and linguistic precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a99416-3156-43a0-b6c8-f76ef1739f8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a69dc26f-5a2b-42e7-9dbb-1321b56f9371",
   "metadata": {},
   "source": [
    "# Q6. Provide a Python code snippet for performing stemming on a list of words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ae8550-4997-4bb4-886a-bcac07f496ce",
   "metadata": {},
   "source": [
    "### Answer:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a14bf9d-1cb5-404b-9094-89c818cf2279",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "def perform_stemming(words):\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_words = [stemmer.stem(word) for word in words]\n",
    "    return stemmed_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5645ff71-dd56-4206-91a9-1ba817111942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['run', 'happili', 'better', 'play']\n"
     ]
    }
   ],
   "source": [
    "words = [\"running\", \"happily\", \"better\", \"playing\"]\n",
    "stemmed_words = perform_stemming(words)\n",
    "print(stemmed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee0f3dc-3328-420c-b29a-8cf4d8d0ef36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1657afaf-d082-497c-808c-b6b40f73be79",
   "metadata": {},
   "source": [
    "# Q7. What is the role of text classification in natural language processing? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff21b5b-2348-4b78-b040-db03d56c8634",
   "metadata": {},
   "source": [
    "### Answer:-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52613d5-1006-419b-a62b-2bcf41615961",
   "metadata": {},
   "source": [
    "Text classification is a fundamental task in natural language processing (NLP) that involves assigning predefined categories or labels to pieces of text based on their content. The primary role of text classification is to automatically analyze and categorize unstructured text data, enabling machines to make sense of human language. \n",
    "\n",
    "### Here are key roles and applications of text classification in NLP:\n",
    "\n",
    "#### Document Categorization:\n",
    "\n",
    "Classifying documents into predefined categories or topics. For example, categorizing news articles into sections like politics, sports, or entertainment.\n",
    "#### Sentiment Analysis:\n",
    "\n",
    "Determining the sentiment expressed in a piece of text, such as identifying whether a review is positive, negative, or neutral. This is crucial for understanding user opinions and feedback.\n",
    "#### Spam Detection:\n",
    "\n",
    "Identifying and filtering out spam or unwanted messages from emails, comments, or other forms of user-generated content.\n",
    "#### Topic Modeling:\n",
    "\n",
    "Uncovering latent topics within a collection of documents. Text classification can be used to assign topics to documents, aiding in organizing and summarizing large datasets.\n",
    "#### Intent Recognition:\n",
    "\n",
    "Understanding the intent behind user queries or statements. This is common in chatbots, virtual assistants, and customer support systems.\n",
    "#### Language Identification:\n",
    "\n",
    "Determining the language of a given text. This is useful in multilingual applications and content filtering.\n",
    "#### Named Entity Recognition (NER):\n",
    "\n",
    "Identifying and classifying named entities, such as names of people, organizations, locations, and other specific entities within a text.\n",
    "#### Document Routing:\n",
    "\n",
    "Automatically routing documents to appropriate departments or individuals based on their content. For example, sorting customer support tickets into relevant categories.\n",
    "#### Legal and Compliance Analysis:\n",
    "\n",
    "Analyzing legal documents to categorize them based on legal clauses, topics, or compliance requirements.\n",
    "#### Product Categorization:\n",
    "\n",
    "Categorizing product descriptions or reviews into relevant product categories. This is commonly used in e-commerce applications.\n",
    "#### Medical Text Analysis:\n",
    "\n",
    "Categorizing medical documents, such as patient records or research papers, into specific medical conditions or topics.\n",
    "#### Fraud Detection:\n",
    "\n",
    "Identifying fraudulent activities or transactions by classifying text data associated with financial transactions or communication.\n",
    "\n",
    "Text classification is often approached as a supervised machine learning task, where models are trained on labeled datasets to learn patterns and relationships between the input text and corresponding categories. Common algorithms for text classification include Naive Bayes, Support Vector Machines (SVM), and deep learning approaches such as Convolutional Neural Networks (CNN) and Recurrent Neural Networks (RNN). The effectiveness of text classification models depends on the quality and representativeness of the training data and the chosen features for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31666eaa-b47c-4233-ac1f-313814696100",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "af433530-82d6-4a30-88df-4290c5242fe8",
   "metadata": {},
   "source": [
    "# Q8. Share a basic example of text classification using a machine learning library like scikit-learn in Python. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be455c87-ba18-4cab-a88c-fdc929f1a264",
   "metadata": {},
   "source": [
    "### Answer:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92cf4b27-84a7-4dc3-815d-cbc955f571b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\L184\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.25.0 is required for this version of SciPy (detected version 1.26.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f021389a-d9f2-4308-8afb-f31f9c2398ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\L184\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\movie_reviews.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download the NLTK movie_reviews dataset\n",
    "nltk.download('movie_reviews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2e033db-4eb8-47f0-bd16-9912289f3f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the movie_reviews dataset\n",
    "from nltk.corpus import movie_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2bd5c21-654e-4494-9990-e5e0c99d25e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of documents (reviews) and their corresponding labels (positive or negative)\n",
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "             for category in movie_reviews.categories()\n",
    "             for fileid in movie_reviews.fileids(category)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15a9ba97-c3e1-49bd-9a33-1f813a67e932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the documents\n",
    "import random\n",
    "random.shuffle(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7dd7f3d0-906f-42a8-ae85-891a5a86c962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features (words) and labels\n",
    "all_words = [word.lower() for word in movie_reviews.words()]\n",
    "all_words = nltk.FreqDist(all_words)\n",
    "word_features = list(all_words.keys())[:3000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42aa82d1-26db-429f-8654-3a702552f8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_features(document):\n",
    "    document_words = set(document)\n",
    "    features = {word: (word in document_words) for word in word_features}\n",
    "    return features\n",
    "\n",
    "featuresets = [(document_features(d), c) for (d,c) in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25cb3512-df28-42ca-ba64-8ae1951c2417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_set, test_set = train_test_split(featuresets, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b8c63bf-77a3-433e-bb82-0b9952465eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features and labels from the training set\n",
    "X_train, y_train = zip(*train_set)\n",
    "\n",
    "# Extract features and labels from the test set\n",
    "X_test, y_test = zip(*test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2393302b-18f5-420b-ab2e-b998b13502d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert features to text using a simple join\n",
    "X_train_text = [' '.join(document) for document in X_train]\n",
    "X_test_text = [' '.join(document) for document in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1a0e61a0-f8e3-42e1-a144-57c50af3577b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert features to vectors using CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_vectorized = vectorizer.fit_transform(X_train_text)\n",
    "X_test_vectorized = vectorizer.transform(X_test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0f6615f9-da2e-4ee0-bba7-087ba8d10333",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train a Naive Bayes classifier\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train_vectorized, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5b8d799c-307b-458a-83ce-56bb2c907ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "predictions = classifier.predict(X_test_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a35c59c9-68bd-4512-9a5e-e95d81384d9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 47.50%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the performance\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f\"Accuracy: {accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b5d0c31c-20d0-4174-ab40-aa899758382e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.47      1.00      0.64       190\n",
      "         pos       0.00      0.00      0.00       210\n",
      "\n",
      "    accuracy                           0.48       400\n",
      "   macro avg       0.24      0.50      0.32       400\n",
      "weighted avg       0.23      0.47      0.31       400\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\L184\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\L184\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\L184\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Display the classification report\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6760a33b-e8db-4517-9daf-49f425d2fc6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f737a859-50dd-4870-8a99-0a1166aed439",
   "metadata": {},
   "source": [
    "# Q9. Why is Named Entity Recognition (NER) important, and what types of entities can it identify? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5480f8d5-2a68-470c-b5b0-59b97fcd86f1",
   "metadata": {},
   "source": [
    "### Answer:-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d232f0ae-6a25-49da-bb83-dbf0765e8d1c",
   "metadata": {},
   "source": [
    "Named Entity Recognition (NER) is a crucial task in natural language processing (NLP) because it involves identifying and classifying entities in text, providing valuable information about specific elements mentioned in the text. The primary importance of NER lies in its ability to extract structured information from unstructured text, enabling applications to understand and work with named entities in a meaningful way. \n",
    "\n",
    "### Here are some key reasons why NER is important:\n",
    "\n",
    "#### Information Extraction:\n",
    "\n",
    "NER helps extract structured information from unstructured text, allowing systems to identify and catalog important details such as names of people, organizations, locations, dates, percentages, and more.\n",
    "#### Data Linking and Integration:\n",
    "\n",
    "By recognizing named entities, NER facilitates the linking of text data with external knowledge bases or databases. This linking enables the integration of information and enhances the overall understanding of the content.\n",
    "#### Improved Search and Retrieval:\n",
    "\n",
    "NER enhances search capabilities by enabling users to search for specific entities or categories within a document or a corpus. It contributes to more accurate and relevant search results.\n",
    "#### Event Extraction:\n",
    "\n",
    "NER can be used to identify entities involved in events or activities mentioned in text. This information is valuable for understanding relationships, roles, and interactions between entities.\n",
    "#### Sentiment Analysis:\n",
    "\n",
    "Identifying named entities in text can contribute to more nuanced sentiment analysis. Knowing which entities are mentioned in positive or negative contexts can provide a deeper understanding of opinions and sentiments.\n",
    "#### Enhanced Language Understanding:\n",
    "\n",
    "NER contributes to the overall understanding of language by recognizing and categorizing named entities. This understanding is crucial for various NLP tasks, such as machine translation, summarization, and question-answering systems.\n",
    "#### Legal and Compliance Analysis:\n",
    "\n",
    "In legal documents, contracts, and compliance-related text, NER can identify and categorize entities such as legal provisions, names of parties, dates, and monetary amounts, facilitating legal analysis and compliance monitoring.\n",
    "#### Financial and Business Analysis:\n",
    "\n",
    "In financial reports and business documents, NER can identify entities related to companies, financial figures, dates, and other relevant information, supporting financial analysis and decision-making.\n",
    "\n",
    "### Types of entities that NER can identify include:\n",
    "\n",
    "##### Person:\n",
    "Names of individuals.\n",
    "##### Organization:\n",
    "Names of companies, institutions, or other organized entities.\n",
    "##### Location:\n",
    "Geographical locations, such as cities, countries, and landmarks.\n",
    "##### Date:\n",
    "References to specific dates or date ranges.\n",
    "##### Time:\n",
    "Mentions of specific times or time intervals.\n",
    "##### Percentage:\n",
    "Indications of percentage values.\n",
    "##### Money:\n",
    "References to monetary values or currencies.\n",
    "##### Product:\n",
    "Names of products or items.\n",
    "##### Event:\n",
    "Names of events or occurrences.\n",
    "The ability to recognize and categorize these entities enhances the overall utility of NLP applications, making them more powerful in extracting actionable information from diverse textual data sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb954555-e22f-4623-8fcf-9e3a032ff422",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7e3731b0-fab7-4ebe-bc0c-a47d98b35c97",
   "metadata": {},
   "source": [
    "# Q10. Develop a simple Python function to perform basic Named Entity Recognition on a given text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b1fc70-4682-4b27-8a60-89844d8cebca",
   "metadata": {},
   "source": [
    "### Answer:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f54874bf-bfc8-45ae-aef2-53175beab8f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-3.7.2-cp39-cp39-win_amd64.whl (12.2 MB)\n",
      "     ---------------------------------------- 12.2/12.2 MB 1.1 MB/s eta 0:00:00\n",
      "Collecting thinc<8.3.0,>=8.1.8\n",
      "  Downloading thinc-8.2.1-cp39-cp39-win_amd64.whl (1.5 MB)\n",
      "     ---------------------------------------- 1.5/1.5 MB 946.7 kB/s eta 0:00:00\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\l184\\anaconda3\\lib\\site-packages (from spacy) (2.28.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\l184\\anaconda3\\lib\\site-packages (from spacy) (1.26.2)\n",
      "Collecting weasel<0.4.0,>=0.1.0\n",
      "  Downloading weasel-0.3.4-py3-none-any.whl (50 kB)\n",
      "     -------------------------------------- 50.1/50.1 kB 632.8 kB/s eta 0:00:00\n",
      "Collecting typer<0.10.0,>=0.3.0\n",
      "  Downloading typer-0.9.0-py3-none-any.whl (45 kB)\n",
      "     -------------------------------------- 45.9/45.9 kB 565.4 kB/s eta 0:00:00\n",
      "Requirement already satisfied: setuptools in c:\\users\\l184\\anaconda3\\lib\\site-packages (from spacy) (63.4.1)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4\n",
      "  Downloading pydantic-2.5.2-py3-none-any.whl (381 kB)\n",
      "     ------------------------------------ 381.9/381.9 kB 553.0 kB/s eta 0:00:00\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.8-cp39-cp39-win_amd64.whl (39 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\l184\\anaconda3\\lib\\site-packages (from spacy) (21.3)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1\n",
      "  Downloading wasabi-1.1.2-py3-none-any.whl (27 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.9-cp39-cp39-win_amd64.whl (122 kB)\n",
      "     -------------------------------------- 122.7/122.7 kB 1.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\l184\\anaconda3\\lib\\site-packages (from spacy) (4.64.1)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\l184\\anaconda3\\lib\\site-packages (from spacy) (5.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\l184\\anaconda3\\lib\\site-packages (from spacy) (2.11.3)\n",
      "Collecting srsly<3.0.0,>=2.4.3\n",
      "  Downloading srsly-2.4.8-cp39-cp39-win_amd64.whl (483 kB)\n",
      "     -------------------------------------- 483.8/483.8 kB 1.1 MB/s eta 0:00:00\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
      "  Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.10-cp39-cp39-win_amd64.whl (25 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Collecting langcodes<4.0.0,>=3.2.0\n",
      "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "     ------------------------------------ 181.6/181.6 kB 577.1 kB/s eta 0:00:00\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\l184\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy) (3.0.9)\n",
      "Collecting typing-extensions>=4.6.1\n",
      "  Downloading typing_extensions-4.8.0-py3-none-any.whl (31 kB)\n",
      "Collecting annotated-types>=0.4.0\n",
      "  Downloading annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
      "Collecting pydantic-core==2.14.5\n",
      "  Downloading pydantic_core-2.14.5-cp39-none-win_amd64.whl (1.9 MB)\n",
      "     ---------------------------------------- 1.9/1.9 MB 1.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\l184\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\l184\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\l184\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.9.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\l184\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3)\n",
      "Collecting confection<1.0.0,>=0.0.1\n",
      "  Downloading confection-0.1.4-py3-none-any.whl (35 kB)\n",
      "Collecting blis<0.8.0,>=0.7.8\n",
      "  Downloading blis-0.7.11-cp39-cp39-win_amd64.whl (6.6 MB)\n",
      "     ---------------------------------------- 6.6/6.6 MB 1.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: colorama in c:\\users\\l184\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.5)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\l184\\anaconda3\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.0.4)\n",
      "Collecting colorama\n",
      "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Collecting cloudpathlib<0.17.0,>=0.7.0\n",
      "  Downloading cloudpathlib-0.16.0-py3-none-any.whl (45 kB)\n",
      "     -------------------------------------- 45.0/45.0 kB 561.0 kB/s eta 0:00:00\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\l184\\anaconda3\\lib\\site-packages (from jinja2->spacy) (2.0.1)\n",
      "Installing collected packages: cymem, typing-extensions, spacy-loggers, spacy-legacy, murmurhash, langcodes, colorama, catalogue, blis, annotated-types, wasabi, srsly, pydantic-core, preshed, cloudpathlib, typer, pydantic, confection, weasel, thinc, spacy\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.3.0\n",
      "    Uninstalling typing_extensions-4.3.0:\n",
      "      Successfully uninstalled typing_extensions-4.3.0\n",
      "  Attempting uninstall: colorama\n",
      "    Found existing installation: colorama 0.4.5\n",
      "    Uninstalling colorama-0.4.5:\n",
      "      Successfully uninstalled colorama-0.4.5\n",
      "Successfully installed annotated-types-0.6.0 blis-0.7.11 catalogue-2.0.10 cloudpathlib-0.16.0 colorama-0.4.6 confection-0.1.4 cymem-2.0.8 langcodes-3.3.0 murmurhash-1.0.10 preshed-3.0.9 pydantic-2.5.2 pydantic-core-2.14.5 spacy-3.7.2 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.4.8 thinc-8.2.1 typer-0.9.0 typing-extensions-4.8.0 wasabi-1.1.2 weasel-0.3.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ccb7f29c-e068-43f0-9fc5-f7d52f3e30fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 12.8/12.8 MB 1.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.8.0,>=3.7.2 in c:\\users\\l184\\anaconda3\\lib\\site-packages (from en-core-web-sm==3.7.1) (3.7.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\l184\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (63.4.1)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\l184\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\l184\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.64.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\l184\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\l184\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (21.3)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\l184\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\l184\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\l184\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in c:\\users\\l184\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.1)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\l184\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (5.2.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\l184\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.5.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\l184\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.11.3)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\l184\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\l184\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\l184\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\l184\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\l184\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.28.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\l184\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\users\\l184\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\l184\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\l184\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: pydantic-core==2.14.5 in c:\\users\\l184\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.14.5)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\l184\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.6.0)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\l184\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.8.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\l184\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\l184\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\l184\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2022.9.14)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\l184\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.11)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\l184\\anaconda3\\lib\\site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\l184\\anaconda3\\lib\\site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\l184\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\l184\\anaconda3\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.0.4)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\users\\l184\\anaconda3\\lib\\site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\l184\\anaconda3\\lib\\site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.1)\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.7.1\n",
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bdd86b8e-6bd6-4d43-88be-d5d9a20d8677",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "def spacy_ner(text):\n",
    "    # Load the English NLP model from spaCy\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    # Process the text with spaCy NLP pipeline\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Extract named entities\n",
    "    named_entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "\n",
    "    return named_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0daf6e82-669f-415f-922f-c115d491621e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Apple Inc.', 'ORG'), ('London', 'GPE'), ('2022', 'DATE')]\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "text = \"Apple Inc. is planning to open a new office in London in 2022.\"\n",
    "result = spacy_ner(text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d468b0-bfbf-443a-8f09-6b66648abb88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
